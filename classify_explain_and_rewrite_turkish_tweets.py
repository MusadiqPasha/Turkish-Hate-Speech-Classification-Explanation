# -*- coding: utf-8 -*-
"""Classify Explain and Rewrite Turkish Tweets

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/musadiqpashak/classify-explain-and-rewrite-turkish-tweets.0f29ecef-4f51-4cbe-95ee-9161d44753ea.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250425/auto/storage/goog4_request%26X-Goog-Date%3D20250425T174951Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2d796057587f28e8ff4381147efd6020833f8b9e60517b08972b487677f6069404dacdfdaf177bcef8e7574da196b642348dee264f936608b7a1b9336579deb356138d5936869b33e1f6119990d3f5252509758c5c72587e635f98c4b5582d5b8876b7cc0752d686f904d082a631429ea09a8e07e71c3ce18ff90484e61c7f8a2ea4cf5f325afd130f20695aab5b658d13bc74e6718285f71a0362fcb5b9cd3ba9d366a781e9c7a33ac8e5c417ebffe1997f428b3d196047d56d7fc6848ad7bb0323484e4dc91e85410a89389f027d6bae6bdbfc6b803b4dfcb4733ecbc3811f6a1c15c1a074ca3cc90204427589cadbaf7825938c48ed57b4979c5fcfde2da3
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score
!pip install deep-translator

"""# EDA"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Load the final CSV file
df = pd.read_csv("/kaggle/input/turkish-hatespeech-tweets/tweets_data_16k.csv")
df['tweet'] = df['tweet'].fillna("")
# Compute Tweet Length and Word Count
df['tweet_length'] = df['tweet'].apply(len)
df['word_count'] = df['tweet'].apply(lambda x: len(x.split()))

# Set Seaborn Style
sns.set(style="whitegrid")

# Figure size
plt.figure(figsize=(14, 10))

# 1. Histogram of Tweet Lengths
plt.subplot(2, 3, 1)
sns.histplot(df['tweet_length'], bins=30, kde=True, color='blue')
plt.title('Tweet Length Distribution')
plt.xlabel('Tweet Length')
plt.ylabel('Frequency')

# 2. Histogram of Word Counts
plt.subplot(2, 3, 2)
sns.histplot(df['word_count'], bins=30, kde=True, color='green')
plt.title('Word Count Distribution')
plt.xlabel('Word Count')
plt.ylabel('Frequency')

# 3. Label Distribution
plt.subplot(2, 3, 3)
sns.countplot(y=df['label'], order=df['label'].value_counts().index, palette='coolwarm')
plt.title('Label Distribution')
plt.xlabel('Count')
plt.ylabel('Label')

# 4. Boxplot of Tweet Lengths per Label
plt.subplot(2, 3, 4)
sns.boxplot(x='label', y='tweet_length', data=df, palette='magma')
plt.title('Tweet Length by Label')
plt.xlabel('Label')
plt.ylabel('Tweet Length')

# 5. Boxplot of Word Count per Label
plt.subplot(2, 3, 5)
sns.boxplot(x='label', y='word_count', data=df, palette='coolwarm')
plt.title('Word Count by Label')
plt.xlabel('Label')
plt.ylabel('Word Count')

# Adjust Layout
plt.tight_layout()

# Save the visualizations
plt.savefig("eda_metrics_visualization.png", dpi=300)
plt.show()

# 6. Word Cloud for Most Common Words
plt.figure(figsize=(10, 5))
text = " ".join(tweet for tweet in df["tweet"])
wordcloud = WordCloud(width=800, height=400, background_color="black").generate(text)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Most Common Words in Tweets")
plt.savefig("wordcloud.png", dpi=300)
plt.show()

# Print Summary Stats
print(f"Total Tweets: {len(df)}")
print(f"Label Counts:\n{df['label'].value_counts()}")
print(f"Average Tweet Length: {df['tweet_length'].mean():.2f}")
print(f"Average Word Count: {df['word_count'].mean():.2f}")

import numpy as np # linear algebra
import os
import pandas as pd
import json
import os
import torch.nn.functional as F
from pprint import pprint
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
import warnings
from datasets import load_dataset, Dataset
from huggingface_hub import notebook_login
from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report
from huggingface_hub import notebook_login
from huggingface_hub import login

os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

"""# 1. Model and Tokenizer Setup

1. Loads the **multilingual BERT model** (`bert-base-multilingual-cased`) using `AutoModelForSequenceClassification` with **2 output labels** for classification.

2. Applies **4-bit quantization** via `BitsAndBytesConfig` to enable **efficient, memory-optimized training**, using the **nf4 quantization type** and **bfloat16** for computation.

3. Loads and adjusts the **tokenizer**, adds a `[PAD]` token if missing, and uses `prepare_model_for_kbit_training` to make the model ready for **low-bit training**.
"""

model_name = "google-bert/bert-base-multilingual-cased"

# Define BitsAndBytesConfig for 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load the base model for sequence classification (note: we set num_labels=3 for our three classes)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config
)

# Load the tokenizer and ensure a pad token is defined
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))

model = prepare_model_for_kbit_training(model)

print('Done')

"""# 2. PEFT/LoRA Fine-Tuning Configuration

1. Defines the **PEFT (Parameter Efficient Fine-Tuning)** configuration using **LoraConfig** to fine-tune a model efficiently for sequence classification (`SEQ_CLS`), with parameters like **dropout rate** and **adaptation strength**.

2. Applies the **PEFT model** configuration (`peft_config`) to the base model using `get_peft_model`, enabling efficient fine-tuning with specific target modules (`query`, `value`) and saving only the **classifier module**.
"""

peft_config = LoraConfig(
    r=8,  # Increase for better adaptation
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="SEQ_CLS",
    target_modules=["query", "value"],
    modules_to_save=["classifier"]
)


model = get_peft_model(model, peft_config)

print('Done')

"""# 3. Data Preparation with Train/Test Split

1. Loads a **Turkish hate speech dataset** from a CSV file and converts it to a Hugging Face **Dataset** object. It strips extra spaces from column names for consistency.

2. Defines a **label mapping** (`nefret` to 0 and `hiçbiri` to 1) and applies it to the dataset using the `encode_labels` function, which updates the labels accordingly.

3. Tokenizes the **tweets** in the dataset using the **tokenizer**, applying padding, truncation, and limiting the sequence length to 128 tokens.

4. **Splits the data** into **train**, **validation**, and **test** sets with an 80%-10%-10% split. A random seed (`RANDOM_SEED = 42`) ensures reproducibility of the split.

5. **Checks the class distribution** in each dataset split (train, validation, test) by counting the occurrences of each label and printing the distribution percentages.
"""

# Load your Turkish hate speech dataset
df = pd.read_csv("/kaggle/input/turkish-hatespeech-tweets/tweets_data_16k.csv")
df.columns = [str(q).strip() for q in df.columns]
data = Dataset.from_pandas(df)

# Define the label mapping
label_map = {"nefret": 0, "hiçbiri": 1}

def encode_labels(example):
    example["label"] = label_map[example["label"]]
    return example

data = data.map(encode_labels)

# Tokenization function
def tokenize_function(example):
    return tokenizer(
        example["tweet"],
        padding="longest",
        truncation=True,
        max_length=128
    )

data = data.map(tokenize_function, batched=True)

# Set random seed for reproducibility
RANDOM_SEED = 42

# Split data into train (80%), validation (10%), and test (10%)
train_test_split = data.train_test_split(test_size=0.2, seed=RANDOM_SEED)
train_data = train_test_split["train"]
test_valid_data = train_test_split["test"].train_test_split(test_size=0.5, seed=RANDOM_SEED)
valid_data = test_valid_data["train"]
test_data = test_valid_data["test"]

print(f"Train dataset size: {len(train_data)}")
print(f"Validation dataset size: {len(valid_data)}")
print(f"Test dataset size: {len(test_data)}")

# Check class distribution in each split
def print_class_distribution(dataset, split_name):
    labels = dataset["label"]
    label_counts = {0: 0, 1: 0}
    for label in labels:
        label_counts[label] += 1

    total = len(labels)
    print(f"\n{split_name} Split Class Distribution:")
    for label, count in label_counts.items():
        label_name = {0: "nefret", 1: "hiçbiri"}[label]
        percentage = (count / total) * 100
        print(f"  {label_name}: {count} ({percentage:.2f}%)")

print_class_distribution(train_data, "Train")
print_class_distribution(valid_data, "Validation")
print_class_distribution(test_data, "Test")

"""# 4. Training Setup

1. Defines **training arguments** using `TrainingArguments`, including settings for batch sizes, number of epochs, learning rate, mixed precision (`bf16`/`fp16`), optimizer, learning rate scheduler, and evaluation strategy. It specifies saving models after each epoch and using **F1 macro** as the evaluation metric.

2. Sets up a **data collator** to dynamically pad sequences during training, ensuring uniform input lengths.

3. Implements a `compute_metrics` function to evaluate the model’s performance by calculating **accuracy**, **F1 macro**, **precision**, **recall**, and detailed **classification report** for both classes (`nefret` and `hiçbiri`).

4. Configures a **Trainer** object with the model, datasets, training arguments, tokenizer, data collator, and metrics.
"""

training_args = transformers.TrainingArguments(
    output_dir="./turkish_hate_speech",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,  # Larger batch size for evaluation
    gradient_accumulation_steps=4,
    num_train_epochs=10,
    learning_rate=1e-4,
    bf16=True if torch.cuda.get_device_capability()[0] >= 8 else False,
    fp16=True if torch.cuda.get_device_capability()[0] < 8 else False,
    optim="paged_adamw_8bit",
    lr_scheduler_type="cosine",
    warmup_ratio=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_f1_macro",  # Using F1 macro instead of loss
    greater_is_better=True,  # F1 score should be maximized
    report_to="none"
)

data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)

    # Calculate basic metrics
    accuracy = accuracy_score(labels, predictions)
    f1_macro = f1_score(labels, predictions, average="macro")
    precision_macro = precision_score(labels, predictions, average="macro")
    recall_macro = recall_score(labels, predictions, average="macro")

    # Get detailed classification report
    report = classification_report(labels, predictions,
                                  target_names=["nefret", "hiçbiri"],
                                  output_dict=True)

    # Return all metrics
    return {
        "accuracy": accuracy,
        "f1_macro": f1_macro,
        "precision_macro": precision_macro,
        "recall_macro": recall_macro,
        "f1_nefret": report["nefret"]["f1-score"],
        "f1_hicbiri": report["hiçbiri"]["f1-score"]
    }

trainer = transformers.Trainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=valid_data,  # Now using validation set for evaluation during training
    args=training_args,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

model.config.use_cache = False
# model.gradient_checkpointing_enable(use_reentrant=False)
# warnings.filterwarnings("ignore", message="torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly")


trainer.train()

"""# 5. Evaluate on Test Set"""

# Evaluate on the test set
test_results = trainer.evaluate(test_data)
print("\nTest Set Evaluation:")
print(f"Test Accuracy: {test_results['eval_accuracy']:.4f}")
print(f"Test F1 Macro: {test_results['eval_f1_macro']:.4f}")
print(f"Test Precision Macro: {test_results['eval_precision_macro']:.4f}")
print(f"Test Recall Macro: {test_results['eval_recall_macro']:.4f}")
print('\n\n')
print("\nClass-specific F1 scores:")
print(f"  'nefret' class: {test_results['eval_f1_nefret']:.4f}")
print(f"  'hiçbiri' class: {test_results['eval_f1_hicbiri']:.4f}")

"""# 6. Inference Example"""

from deep_translator import GoogleTranslator
def classify_text(input_text):
    inputs = tokenizer(input_text, return_tensors="pt").to(device)

    with torch.no_grad():
        logits = model(**inputs).logits
        probs = F.softmax(logits, dim=-1).cpu().numpy().flatten()

    predicted_label = torch.argmax(logits, dim=-1).item()
    label_map_inv = {0: "nefret", 1: "hiçbiri"}

    # Return prediction and confidence scores for all classes
    result = {
        "predicted_label": label_map_inv[predicted_label],
        "confidence": probs[predicted_label],
        "all_probabilities": {
            "nefret": float(probs[0]),
            "hiçbiri": float(probs[1])
        }
    }

    return result

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Test with a few examples
test_texts = [
    "Bu kadar kötü bir şey olabilir mi?",
    "Sizin gibi insanlar bu toplumda hiçbir işe yaramaz, sadece sorun yaratıyorsunuz!",
    "Bugün hava çok güzel, parka gidelim.",
    "sirinyk@USERÂ buna ne demeli?? gÃ¶zÃ¼mde canlandirdim",
    "engintazegul Ulan bu tamam bir Abaza gavat Ã¶nÃ¼ne gelene yaziyor. Sapik gavat ulan san kati kiz kovalayacagima Ã¶nÃ¼ndeki topu kovalasaydin."
]

print("\nInference Examples:")
for text in test_texts:
    result = classify_text(text)
    print(f"\nText: {text}")
    translated = GoogleTranslator(source='tr', target='en').translate(text)
    print("Translated:", translated)
    print(f"Prediction: {result['predicted_label']} (Confidence: {result['confidence']:.4f})")
    print("All class probabilities:")
    for label, prob in result['all_probabilities'].items():
        print(f"  {label}: {prob*100:.4f}")

"""# 7. EXplanability and Alternate Tweet"""

import shap
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from peft import PeftModel
import pandas as pd
from huggingface_hub import login
import numpy as np

def rewrite_turkish_hate_tweet(text):
    # Replace with your actual token (make sure to keep it secret!)
    HF_TOKEN = "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"

    # Log in using the token
    login(token=HF_TOKEN)

    # Load Turkish hate speech classification model
    tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
    base_model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-multilingual-cased")
    model = PeftModel.from_pretrained(base_model, "vanishingradient/turkish_hate_speech")
    classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)

    # Run classification
    result = classifier(text)
    print(f"Classification result: {result}")


    if result[0]['label']=='LABEL_1':
        # SHAP explainer for token-level importance
        explainer = shap.Explainer(classifier)
        shap_values = explainer([text])
        sv = shap_values[0]

         # Visualize explanation
        shap.plots.text(shap_values[0])
        return ''

    # SHAP explainer for token-level importance
    explainer = shap.Explainer(classifier)
    shap_values = explainer([text])
    sv = shap_values[0]

    # Extract top hate-contributing words
    token_contributions = {}
    for token, value in zip(sv.data, sv.values):
        if token.strip() and value[0] >= 0.005:
            token_contributions[token] = value[0]

    sorted_words = sorted(token_contributions.items(), key=lambda x: x[1], reverse=True)
    hate_words = [word for word, value in sorted_words[:10]]  # top 10

    print(f"Top words contributing to hate speech classification: {hate_words}")

    # Visualize explanation
    shap.plots.text(shap_values[0])

    # Craft prompt for LLaMA
    messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant that rewrites social media posts in Turkish by removing any hate speech, slurs, or offensive content while preserving the original meaning and tone."
        },
        {
            "role": "user",
            "content": f"""Original text: {text}

            Words to replace: {', '.join(hate_words)}

            Please rewrite the text, replacing only these words with more respectful alternatives. Keep the same meaning and tone otherwise. Avoid changing neutral or emotional expressions that are not part of the offensive list."""
        }
    ]

    # Generate cleaned text using LLaMA
    pipe = pipeline("text-generation",
                    model="meta-llama/Llama-3.2-3B-Instruct",
                    max_new_tokens=200,
                    temperature=0.2)

    response = pipe(messages)
    print("\nRewritten text:")
    print(response[0]['generated_text'])

    # Try to extract assistant's response (if it's formatted as a role-based dict)
    assistant_response = None
    for item in response[0]['generated_text']:
        if isinstance(item, dict) and item.get('role') == 'assistant':
            assistant_response = item.get('content')
            break

    return assistant_response if assistant_response else response[0]['generated_text']

"""# 8 Ineference Example [ NOT HATEFUL ]"""

## nefret  [HATEFUL] : 0
## hicbiri [NOT HATEFUL]: 1

text='''Bide ÅŸu kendini ingiliz sanan tayfa varya. Kiza ismini sorsak ingilizce cevap veremez hesabina bi bakiyosun hooobaaaaaa heryer ingilizce sÃ¶z. Komiksiniz BE BE GIMMðŸ˜‰ðŸ˜‰'''

cleaned = rewrite_turkish_hate_tweet(text)
if cleaned:
    print("\n✅ Final Cleaned Tweet:\n", cleaned)

text = '''Sabahları alarm çalınca gözümü açıp beş dakika daha diyerek bütün hayatımı erteliyorum sanırım, çünkü her gün aynı döngüde uyanıp yine geç kalıyorum ama hiçbir şey değişmiyor.'''

cleaned = rewrite_turkish_hate_tweet(text)
if cleaned:
    print("\n✅ Final Cleaned Tweet:\n", cleaned)

text='''#CananKaftancÄ±oÄŸlu nu iÃ§eri atacaklar Ä°BB ye belki kayyum atayacaklar veya o korkuyu sÃ¼rekli hissettirecekler KÃ¼rt bÃ¶lgeleri zaten zaptedilmiÅŸ peki biz TÃ¼rkiye halklarÄ± olarak ne yapacaÄŸÄ±z. Ä¶Ä±lÄ±Ã§daroÄŸlu gibi "bÃ¶yle bir ÅŸey olabilir mi arkadaÅŸlar" mÄ± diyeceÄŸiz?

 #Susamam'''

cleaned = rewrite_turkish_hate_tweet(text)
if cleaned:
    print("\n✅ Final Cleaned Tweet:\n", cleaned)

"""# 9 Ineference Example [ HATEFUL ]"""

text = "ya orospu cocuklari hepiniz niye ayni anda yaziyonuz tam ciddi iliski yapcam biri geliyo surprise motherfucker diye cikiyo sonra suriyeli gibi uruyolar aklim karisiyo"
cleaned = rewrite_turkish_hate_tweet(text)
if cleaned:
    print("\n✅ Final Cleaned Tweet:\n", cleaned)

text ="Allah askina git allah'im duy sesimizi Dirar siktir git takimdan"
cleaned = rewrite_turkish_hate_tweet(text)
if cleaned:
    print("\n✅ Final Cleaned Tweet:\n", cleaned)

text ="duygu kakan Serefsiz pislik ?"
cleaned = rewrite_turkish_hate_tweet(text)
if cleaned:
    print("\n✅ Final Cleaned Tweet:\n", cleaned)

text=""""#Hocali'da 29 yil Ã¶nce yasanan vahset yÃ¼rekleri daglamaya devam ediyor. Umuyorum ki, Ermeni esareti altindaki ezeli Azerbaycan topraklarinin azat edilmesi bu acili yÃ¼reklere bir nebze olsun teselli olmustur.
Ermeni zulmÃ¼ne kurban giden sehitlerimizin ruhlari sad olsun."""""

if cleaned:
    print("\n✅ Final Cleaned Tweet:\n", cleaned)